{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBP-JbR-s00X"
   },
   "outputs": [],
   "source": [
    "# Загрузка библиотек\n",
    "import numpy as np # для работы с массивами\n",
    "import pandas as pd # для работы с DataFrame \n",
    "from sklearn import datasets # для импорта данных\n",
    "import seaborn as sns # библиотека для визуализации статистических данных\n",
    "import matplotlib.pyplot as plt # для построения графиков\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kXAjM2H6s01j"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# загружаем датасет\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m boston \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_boston()\n\u001b[0;32m      3\u001b[0m bostonDF \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(boston\u001b[38;5;241m.\u001b[39mdata, columns\u001b[38;5;241m=\u001b[39mboston\u001b[38;5;241m.\u001b[39mfeature_names)\n\u001b[0;32m      4\u001b[0m bostonDF[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRICE\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m boston\u001b[38;5;241m.\u001b[39mtarget\n",
      "File \u001b[1;32md:\\anaconda3\\Lib\\site-packages\\sklearn\\datasets\\__init__.py:156\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_boston\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    106\u001b[0m     msg \u001b[38;5;241m=\u001b[39m textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m        `load_boston` has been removed from scikit-learn since version 1.2.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     )\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m()[name]\n",
      "\u001b[1;31mImportError\u001b[0m: \n`load_boston` has been removed from scikit-learn since version 1.2.\n\nThe Boston housing prices dataset has an ethical problem: as\ninvestigated in [1], the authors of this dataset engineered a\nnon-invertible variable \"B\" assuming that racial self-segregation had a\npositive impact on house prices [2]. Furthermore the goal of the\nresearch that led to the creation of this dataset was to study the\nimpact of air quality but it did not give adequate demonstration of the\nvalidity of this assumption.\n\nThe scikit-learn maintainers therefore strongly discourage the use of\nthis dataset unless the purpose of the code is to study and educate\nabout ethical issues in data science and machine learning.\n\nIn this special case, you can fetch the dataset from the original\nsource::\n\n    import pandas as pd\n    import numpy as np\n\n    data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n    raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n    data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n    target = raw_df.values[1::2, 2]\n\nAlternative datasets include the California housing dataset and the\nAmes housing dataset. You can load the datasets as follows::\n\n    from sklearn.datasets import fetch_california_housing\n    housing = fetch_california_housing()\n\nfor the California housing dataset and::\n\n    from sklearn.datasets import fetch_openml\n    housing = fetch_openml(name=\"house_prices\", as_frame=True)\n\nfor the Ames housing dataset.\n\n[1] M Carlisle.\n\"Racist data destruction?\"\n<https://medium.com/@docintangible/racist-data-destruction-113e3eff54a8>\n\n[2] Harrison Jr, David, and Daniel L. Rubinfeld.\n\"Hedonic housing prices and the demand for clean air.\"\nJournal of environmental economics and management 5.1 (1978): 81-102.\n<https://www.researchgate.net/publication/4974606_Hedonic_housing_prices_and_the_demand_for_clean_air>\n"
     ]
    }
   ],
   "source": [
    "# загружаем датасет\n",
    "boston = datasets.load_boston()\n",
    "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "bostonDF[\"PRICE\"] = boston.target\n",
    "bostonDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vF0xrPYhs03O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CRIM: Per capita crime rate by town\n",
    "# ZN: Proportion of residential land zoned for lots over 25,000 sq. ft\n",
    "# INDUS: Proportion of non-retail business acres per town\n",
    "# CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "# NOX: Nitric oxide concentration (parts per 10 million)\n",
    "# RM: Average number of rooms per dwelling\n",
    "# AGE: Proportion of owner-occupied units built prior to 1940\n",
    "# DIS: Weighted distances to five Boston employment centers\n",
    "# RAD: Index of accessibility to radial highways\n",
    "# TAX: Full-value property tax rate per $10,000\n",
    "# PTRATIO: Pupil-teacher ratio by town\n",
    "# B: 1000(Bk — 0.63)², where Bk is the proportion of [people of African American descent] by town\n",
    "# LSTAT: Percentage of lower status of the population\n",
    "# MEDV: Median value of owner-occupied homes in $1000s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q1DRX1Pms038"
   },
   "outputs": [],
   "source": [
    "# Хотим узнать, как обращаться к столбцам bostonDF\n",
    "bostonDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVuzLNwNs04z"
   },
   "outputs": [],
   "source": [
    "# полная матрица корреляций\n",
    "# используем метод Pandas corr()\n",
    "C = bostonDF.corr(method=\"pearson\")\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V_KsJ0SEs05-"
   },
   "outputs": [],
   "source": [
    "# представим корреляционную матрицу в виде \"тепловой карты\" с помощью функции heatmap из библиотеки seaborn\n",
    "plt.figure(figsize=(16, 6))  # размер графика\n",
    "sns.heatmap(data=C, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "aRf3F6IHs06W"
   },
   "source": [
    "Строим регрессию из видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZLeejsBs07z"
   },
   "outputs": [],
   "source": [
    "Data = bostonDF[[\"CRIM\", \"RM\"]]\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hlQK1ow9s08Q"
   },
   "outputs": [],
   "source": [
    "np.shape(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZjdiyfbs09r"
   },
   "outputs": [],
   "source": [
    "# Создаем вектор из единиц для коэффициента w_0 и записываем все векторы в СТОЛБЦЫ матрицы признаков А\n",
    "CRIM = Data[\"CRIM\"]\n",
    "RM = Data[\"RM\"]\n",
    "A = np.column_stack((np.ones(506), CRIM, RM))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YAVjH_Os0-C"
   },
   "outputs": [],
   "source": [
    "# Добавим настройку для удобного чтения значений А\n",
    "np.set_printoptions(suppress=True)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VOMIB-As0_B"
   },
   "outputs": [],
   "source": [
    "# Создаем целевой вектор\n",
    "y = bostonDF[[\"PRICE\"]]  # объект типа dataframe - то, что нужно\n",
    "y_s = bostonDF[\"PRICE\"]  #  объект типа series - не подойдет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrychO1Xs0_s"
   },
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HS0RSpGvs1At"
   },
   "outputs": [],
   "source": [
    "type(y_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VrWaTdBas1BU"
   },
   "outputs": [],
   "source": [
    "# вычислим OLS оценку для коэффициентов\n",
    "w_hat = np.linalg.inv(A.T @ A) @ A.T @ y\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2lyMBxws1Bk"
   },
   "outputs": [],
   "source": [
    "# прогноз\n",
    "# добавились данные по новому городку:\n",
    "CRIM_new = 0.1\n",
    "RM_new = 8\n",
    "# делаем прогноз типичной соимости дома\n",
    "PRICE_new = w_hat.iloc[0] + w_hat.iloc[1] * CRIM_new + w_hat.iloc[2] * RM_new\n",
    "PRICE_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DC-NbrL3s1CA"
   },
   "outputs": [],
   "source": [
    "# короткий способ сделать прогноз\n",
    "new = np.array([1, CRIM_new, RM_new])\n",
    "new @ w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h55Xvqmfs1C1"
   },
   "outputs": [],
   "source": [
    "# классическая OLS регрессия в numpy одной командой\n",
    "np.linalg.lstsq(A, y, rcond=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "8fOhMg0ns1DZ"
   },
   "source": [
    "Стандартизация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wJ3PpaYbs1Df"
   },
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N9Drcrd9s1D_"
   },
   "outputs": [],
   "source": [
    "# метод .mean() позволяет вычислить арифметическое среднее значение вектора\n",
    "meanCRIM = Data[\"CRIM\"].mean()\n",
    "meanRM = Data[\"RM\"].mean()\n",
    "mean_y = y.mean()\n",
    "print(\"mean value of CRIME:\", meanCRIM)\n",
    "print(\"mean value of RM:\", meanRM)\n",
    "print(\"mean value of PRICE:\", mean_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e860o4XFs1Ej"
   },
   "outputs": [],
   "source": [
    "# Центрирование\n",
    "CRIM_c = Data[\"CRIM\"] - meanCRIM\n",
    "RM_c = Data[\"RM\"] - meanRM\n",
    "y_c = y - mean_y\n",
    "print(\"CRIME до центрирования:\\n\", CRIM.head(4))\n",
    "print(\"CRIME после центрирования:\\n\", CRIM_c.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ms_nT0CDs1FL"
   },
   "outputs": [],
   "source": [
    "print(\"среднее арифметическое вектора CRIM после центрирования:\", CRIM_c.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2GCWzRgs1GN"
   },
   "outputs": [],
   "source": [
    "# вычисляем длины векторов для нормирования\n",
    "CRIM_c_norm = np.linalg.norm(CRIM_c)\n",
    "RM_c_norm = np.linalg.norm(RM_c)\n",
    "y_c_norm = np.linalg.norm(y_c)\n",
    "print(\"norm of CRIME:\\n\", CRIM_c_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WzUFGCEbs1Ge"
   },
   "outputs": [],
   "source": [
    "# Нормирование: делим каждый центрированный вектор на его длину\n",
    "CRIM_st = CRIM_c / CRIM_c_norm\n",
    "RM_st = RM_c / RM_c_norm\n",
    "y_st = y_c / y_c_norm\n",
    "print(\"CRIME до центрирования:\\n\", CRIM.head(4))\n",
    "print(\"CRIME после центрирования:\\n\", CRIM_c.head(4))\n",
    "print(\"CRIME после нормирования:\\n\", CRIM_st.head(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvqOiLKEs1G1"
   },
   "outputs": [],
   "source": [
    "# Матрица центрированных признаков - БЕЗ константы!\n",
    "A_st = np.column_stack(\n",
    "    (\n",
    "        CRIM_st,\n",
    "        RM_st,\n",
    "    )\n",
    ")\n",
    "A_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ipk6QmAs1IB"
   },
   "outputs": [],
   "source": [
    "# OLS оценка коэффициентов центрированной регрессии\n",
    "w_hat_st = np.linalg.inv(A_st.T @ A_st) @ A_st.T @ y_st.values\n",
    "w_hat_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VQ98KKRgs1IN"
   },
   "outputs": [],
   "source": [
    "# добавились данные по новому городку:\n",
    "CRIM_new = 0.1\n",
    "RM_new = 8\n",
    "# чтобы сделать прогноз по новым данным, их тоже нужно стандартизировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJ99mXE9s1I1"
   },
   "outputs": [],
   "source": [
    "# Стандартизация новых данных\n",
    "CRIM_new_st = (CRIM_new - meanCRIM) / CRIM_c_norm\n",
    "RM_new_st = (RM_new - meanRM) / RM_c_norm\n",
    "print(\"new CRIME st:\", CRIM_new_st)\n",
    "print(\"new RM st:\", RM_new_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e7FGiVXWs1I8"
   },
   "outputs": [],
   "source": [
    "# Прогноз стандартизированного y\n",
    "y_st_new = w_hat_st[0] * CRIM_new_st + w_hat_st[1] * RM_new_st\n",
    "print(\"new PRICE st predict:\", y_st_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ynFUkO1Ys1JO"
   },
   "source": [
    "Стандартизированный прогноз для нас может не иметь никакого смысла сам по себе, \n",
    "\n",
    "поэтому его необходимо пересчитать обратно.\n",
    "\n",
    "Для этого сделаем операции, обратные стандартизации - умножим на длину центрированного вектора y и прибавим среднее\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZULrZ0Qs1JU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Пересчет стандартизированного прогноза в понятный\n",
    "y_new = y_st_new * y_c_norm + mean_y\n",
    "print(\"new PRICE predict:\", y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pdXywY2Fs1Jt"
   },
   "outputs": [],
   "source": [
    "# Пересчет стандартизированных коэффициентов в обычные\n",
    "# здесь создаем вектор из единиц, который далее заполним нужными значениями\n",
    "w_hat_not_st = np.ones((3, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bT7zgk0Ws1Kd"
   },
   "outputs": [],
   "source": [
    "# Пересчет стандартизированных  коэффициентов в обычные\n",
    "w_hat_not_st[0] = (\n",
    "    -w_hat_st[0] * meanCRIM / CRIM_c_norm - w_hat_st[1] * meanRM / RM_c_norm\n",
    ") * y_c_norm + y.mean()\n",
    "w_hat_not_st[1] = (w_hat_st[0] / CRIM_c_norm) * y_c_norm\n",
    "w_hat_not_st[2] = (w_hat_st[1] / RM_c_norm) * y_c_norm\n",
    "w_hat_not_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D8H32-1Ts1Kr"
   },
   "outputs": [],
   "source": [
    "# Сравнение с ранее полученными обычными коэффициентами\n",
    "w_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sqoZSTGVs1LP"
   },
   "outputs": [],
   "source": [
    "# Матрица Грама стандартизированных признаков\n",
    "A_st.T @ A_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "afRFOtUbs1LY"
   },
   "outputs": [],
   "source": [
    "# Матрица корреляций обычных признаков\n",
    "Data.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEKX9Lkes1Lg"
   },
   "outputs": [],
   "source": [
    "# Стандартизированные признаки ортогональны вектору констант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQdYlM1Is1Ln"
   },
   "outputs": [],
   "source": [
    "CRIM_st @ np.ones(506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ICQKPAgqs1MM"
   },
   "outputs": [],
   "source": [
    "RM_st @ np.ones(506)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Ow6Xez6s1MV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "OLS_classic_screencast.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
